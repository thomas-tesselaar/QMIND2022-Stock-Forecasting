{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fe26921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"../DataPreprocessing\")\n",
    "# from DataPreprocessing \n",
    "import data_preprocessing as data_prep\n",
    "# sys.path.append(\"../Data\")\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c93a6ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "  stock_name = 'TSLA'\n",
    "  search_names = ['Term'+str(i) for i in range(1,8)]\n",
    "  main_df = data_prep.merge_csv(stock_name, search_names, search_suffix = '', pathname = \"../Modelling/\", print_head = False, stock_rows_to_drop=[0,1,522])\n",
    "  data_prep.derive_columns(main_df, len(search_names), print_head = False)\n",
    "  clean_df = data_prep.prep_for_learning(main_df, normalize = 'min-max')\n",
    "\n",
    "  test_df = clean_df.sample(frac=(1/4))\n",
    "\n",
    "  train_df = clean_df.drop(index = test_df.index)\n",
    "\n",
    "  test_y = test_df[\"Price Movement\"].to_numpy()\n",
    "  test_x = test_df.drop(columns = \"Price Movement\").to_numpy()\n",
    "\n",
    "  train_y = train_df[\"Price Movement\"].to_numpy()\n",
    "  train_x = train_df.drop(columns = \"Price Movement\").to_numpy()\n",
    "  \n",
    "  return test_x, test_y, train_x, train_y\n",
    "\n",
    "def fit_data(train_x, train_y):\n",
    "  clf = MLPClassifier(hidden_layer_sizes=(6,5),\n",
    "  random_state=5,\n",
    "  verbose=True,\n",
    "  learning_rate_init=0.01)\n",
    "\n",
    "  clf.fit(train_x, train_y)\n",
    "  return clf\n",
    "  \n",
    "def predict(test_x, test_y, train_x, train_y):\n",
    "  clf = fit_data(train_x, train_y)\n",
    "  ypred = clf.predict(test_x)\n",
    "  a_score = accuracy_score(test_y, ypred)\n",
    "  return a_score, ypred, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b213832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sf_0</th>\n",
       "      <th>sf_1</th>\n",
       "      <th>sf_2</th>\n",
       "      <th>sf_3</th>\n",
       "      <th>sf_4</th>\n",
       "      <th>sf_5</th>\n",
       "      <th>sf_6</th>\n",
       "      <th>Search Change 0</th>\n",
       "      <th>Search Movement 0</th>\n",
       "      <th>Search Change 1</th>\n",
       "      <th>...</th>\n",
       "      <th>5 week avg 6</th>\n",
       "      <th>10 week avg 6</th>\n",
       "      <th>One Week Ago Search 0</th>\n",
       "      <th>One Week Ago Search 1</th>\n",
       "      <th>One Week Ago Search 2</th>\n",
       "      <th>One Week Ago Search 3</th>\n",
       "      <th>One Week Ago Search 4</th>\n",
       "      <th>One Week Ago Search 5</th>\n",
       "      <th>One Week Ago Search 6</th>\n",
       "      <th>Price Movement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.318182</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061813</td>\n",
       "      <td>0.068389</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.065856</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.045455</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058379</td>\n",
       "      <td>0.064590</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.045455</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057692</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054945</td>\n",
       "      <td>0.060790</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sf_0  sf_1  sf_2  sf_3  sf_4  sf_5  sf_6  Search Change 0  \\\n",
       "0  0.85  0.02  0.39  0.10  0.36  0.10  0.05        -0.318182   \n",
       "1  0.90  0.02  0.37  0.10  0.36  0.09  0.04         0.227273   \n",
       "2  0.89  0.02  0.35  0.14  0.38  0.07  0.04        -0.045455   \n",
       "3  0.88  0.02  0.37  0.12  0.40  0.06  0.04        -0.045455   \n",
       "4  0.92  0.02  0.31  0.11  0.40  0.07  0.03         0.181818   \n",
       "\n",
       "   Search Movement 0  Search Change 1  ...  5 week avg 6  10 week avg 6  \\\n",
       "0               -1.0              0.0  ...      0.061813       0.068389   \n",
       "1                1.0              0.0  ...      0.059524       0.065856   \n",
       "2               -1.0              0.0  ...      0.058379       0.064590   \n",
       "3               -1.0              0.0  ...      0.057692       0.063830   \n",
       "4                1.0              0.0  ...      0.054945       0.060790   \n",
       "\n",
       "   One Week Ago Search 0  One Week Ago Search 1  One Week Ago Search 2  \\\n",
       "0                   0.92                   0.02                   0.38   \n",
       "1                   0.85                   0.02                   0.39   \n",
       "2                   0.90                   0.02                   0.37   \n",
       "3                   0.89                   0.02                   0.35   \n",
       "4                   0.88                   0.02                   0.37   \n",
       "\n",
       "   One Week Ago Search 3  One Week Ago Search 4  One Week Ago Search 5  \\\n",
       "0                   0.13                   0.40                   0.08   \n",
       "1                   0.10                   0.36                   0.10   \n",
       "2                   0.10                   0.36                   0.09   \n",
       "3                   0.14                   0.38                   0.07   \n",
       "4                   0.12                   0.40                   0.06   \n",
       "\n",
       "   One Week Ago Search 6  Price Movement  \n",
       "0                   0.04             1.0  \n",
       "1                   0.05            -1.0  \n",
       "2                   0.04             1.0  \n",
       "3                   0.04             1.0  \n",
       "4                   0.04             1.0  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.74493453\n",
      "Iteration 2, loss = 0.71307850\n",
      "Iteration 3, loss = 0.69564737\n",
      "Iteration 4, loss = 0.69674337\n",
      "Iteration 5, loss = 0.69599061\n",
      "Iteration 6, loss = 0.68997919\n",
      "Iteration 7, loss = 0.68275997\n",
      "Iteration 8, loss = 0.68042434\n",
      "Iteration 9, loss = 0.67857169\n",
      "Iteration 10, loss = 0.67773263\n",
      "Iteration 11, loss = 0.67537499\n",
      "Iteration 12, loss = 0.67319677\n",
      "Iteration 13, loss = 0.67141464\n",
      "Iteration 14, loss = 0.66991349\n",
      "Iteration 15, loss = 0.66859864\n",
      "Iteration 16, loss = 0.66747715\n",
      "Iteration 17, loss = 0.66617663\n",
      "Iteration 18, loss = 0.66453338\n",
      "Iteration 19, loss = 0.66324462\n",
      "Iteration 20, loss = 0.66253019\n",
      "Iteration 21, loss = 0.66129762\n",
      "Iteration 22, loss = 0.66057319\n",
      "Iteration 23, loss = 0.65926370\n",
      "Iteration 24, loss = 0.65804671\n",
      "Iteration 25, loss = 0.65665511\n",
      "Iteration 26, loss = 0.65554841\n",
      "Iteration 27, loss = 0.65443514\n",
      "Iteration 28, loss = 0.65335749\n",
      "Iteration 29, loss = 0.65217289\n",
      "Iteration 30, loss = 0.65133523\n",
      "Iteration 31, loss = 0.65058781\n",
      "Iteration 32, loss = 0.64959905\n",
      "Iteration 33, loss = 0.64811661\n",
      "Iteration 34, loss = 0.64745216\n",
      "Iteration 35, loss = 0.64665234\n",
      "Iteration 36, loss = 0.64467231\n",
      "Iteration 37, loss = 0.64351734\n",
      "Iteration 38, loss = 0.64312389\n",
      "Iteration 39, loss = 0.64148282\n",
      "Iteration 40, loss = 0.64031458\n",
      "Iteration 41, loss = 0.63908369\n",
      "Iteration 42, loss = 0.63857326\n",
      "Iteration 43, loss = 0.63650868\n",
      "Iteration 44, loss = 0.63521190\n",
      "Iteration 45, loss = 0.63383317\n",
      "Iteration 46, loss = 0.63244475\n",
      "Iteration 47, loss = 0.63195204\n",
      "Iteration 48, loss = 0.62944834\n",
      "Iteration 49, loss = 0.62810566\n",
      "Iteration 50, loss = 0.62735457\n",
      "Iteration 51, loss = 0.62538093\n",
      "Iteration 52, loss = 0.62382112\n",
      "Iteration 53, loss = 0.62262667\n",
      "Iteration 54, loss = 0.62126174\n",
      "Iteration 55, loss = 0.61875567\n",
      "Iteration 56, loss = 0.61636534\n",
      "Iteration 57, loss = 0.61439184\n",
      "Iteration 58, loss = 0.61256557\n",
      "Iteration 59, loss = 0.61087255\n",
      "Iteration 60, loss = 0.60947419\n",
      "Iteration 61, loss = 0.60624665\n",
      "Iteration 62, loss = 0.60682929\n",
      "Iteration 63, loss = 0.60500320\n",
      "Iteration 64, loss = 0.60068129\n",
      "Iteration 65, loss = 0.59846441\n",
      "Iteration 66, loss = 0.59726531\n",
      "Iteration 67, loss = 0.59243514\n",
      "Iteration 68, loss = 0.58954842\n",
      "Iteration 69, loss = 0.58911008\n",
      "Iteration 70, loss = 0.58758326\n",
      "Iteration 71, loss = 0.58319889\n",
      "Iteration 72, loss = 0.58222462\n",
      "Iteration 73, loss = 0.58107038\n",
      "Iteration 74, loss = 0.57938404\n",
      "Iteration 75, loss = 0.57452317\n",
      "Iteration 76, loss = 0.57249236\n",
      "Iteration 77, loss = 0.57043434\n",
      "Iteration 78, loss = 0.56809526\n",
      "Iteration 79, loss = 0.56560321\n",
      "Iteration 80, loss = 0.56398121\n",
      "Iteration 81, loss = 0.56071242\n",
      "Iteration 82, loss = 0.55811645\n",
      "Iteration 83, loss = 0.56057094\n",
      "Iteration 84, loss = 0.56016506\n",
      "Iteration 85, loss = 0.55422327\n",
      "Iteration 86, loss = 0.55395781\n",
      "Iteration 87, loss = 0.55024623\n",
      "Iteration 88, loss = 0.54869588\n",
      "Iteration 89, loss = 0.54743780\n",
      "Iteration 90, loss = 0.54558810\n",
      "Iteration 91, loss = 0.54264821\n",
      "Iteration 92, loss = 0.53889521\n",
      "Iteration 93, loss = 0.53981484\n",
      "Iteration 94, loss = 0.53792511\n",
      "Iteration 95, loss = 0.53877231\n",
      "Iteration 96, loss = 0.53448527\n",
      "Iteration 97, loss = 0.53468428\n",
      "Iteration 98, loss = 0.53203492\n",
      "Iteration 99, loss = 0.52964734\n",
      "Iteration 100, loss = 0.52753537\n",
      "Iteration 101, loss = 0.52565911\n",
      "Iteration 102, loss = 0.52665110\n",
      "Iteration 103, loss = 0.52388102\n",
      "Iteration 104, loss = 0.52385646\n",
      "Iteration 105, loss = 0.52253159\n",
      "Iteration 106, loss = 0.52043590\n",
      "Iteration 107, loss = 0.52168278\n",
      "Iteration 108, loss = 0.51710391\n",
      "Iteration 109, loss = 0.51739971\n",
      "Iteration 110, loss = 0.51822345\n",
      "Iteration 111, loss = 0.51675445\n",
      "Iteration 112, loss = 0.51498970\n",
      "Iteration 113, loss = 0.51340521\n",
      "Iteration 114, loss = 0.51195546\n",
      "Iteration 115, loss = 0.51052326\n",
      "Iteration 116, loss = 0.50902761\n",
      "Iteration 117, loss = 0.50773689\n",
      "Iteration 118, loss = 0.50625043\n",
      "Iteration 119, loss = 0.50662644\n",
      "Iteration 120, loss = 0.50728707\n",
      "Iteration 121, loss = 0.50589378\n",
      "Iteration 122, loss = 0.50791767\n",
      "Iteration 123, loss = 0.50662702\n",
      "Iteration 124, loss = 0.50387836\n",
      "Iteration 125, loss = 0.51682862\n",
      "Iteration 126, loss = 0.50130982\n",
      "Iteration 127, loss = 0.51002390\n",
      "Iteration 128, loss = 0.51245464\n",
      "Iteration 129, loss = 0.50039405\n",
      "Iteration 130, loss = 0.50423247\n",
      "Iteration 131, loss = 0.49838623\n",
      "Iteration 132, loss = 0.50027578\n",
      "Iteration 133, loss = 0.49727687\n",
      "Iteration 134, loss = 0.49606570\n",
      "Iteration 135, loss = 0.49257411\n",
      "Iteration 136, loss = 0.49370049\n",
      "Iteration 137, loss = 0.49036164\n",
      "Iteration 138, loss = 0.49137061\n",
      "Iteration 139, loss = 0.48868366\n",
      "Iteration 140, loss = 0.48931280\n",
      "Iteration 141, loss = 0.49003626\n",
      "Iteration 142, loss = 0.48871279\n",
      "Iteration 143, loss = 0.48716432\n",
      "Iteration 144, loss = 0.48706025\n",
      "Iteration 145, loss = 0.48542068\n",
      "Iteration 146, loss = 0.48660625\n",
      "Iteration 147, loss = 0.48427502\n",
      "Iteration 148, loss = 0.48437966\n",
      "Iteration 149, loss = 0.48341002\n",
      "Iteration 150, loss = 0.48459712\n",
      "Iteration 151, loss = 0.48407249\n",
      "Iteration 152, loss = 0.48332260\n",
      "Iteration 153, loss = 0.48066067\n",
      "Iteration 154, loss = 0.47915303\n",
      "Iteration 155, loss = 0.47914698\n",
      "Iteration 156, loss = 0.47807897\n",
      "Iteration 157, loss = 0.48222978\n",
      "Iteration 158, loss = 0.47686045\n",
      "Iteration 159, loss = 0.47949018\n",
      "Iteration 160, loss = 0.47567580\n",
      "Iteration 161, loss = 0.47825440\n",
      "Iteration 162, loss = 0.47532147\n",
      "Iteration 163, loss = 0.47701688\n",
      "Iteration 164, loss = 0.47751546\n",
      "Iteration 165, loss = 0.47817498\n",
      "Iteration 166, loss = 0.47993118\n",
      "Iteration 167, loss = 0.47857111\n",
      "Iteration 168, loss = 0.47990670\n",
      "Iteration 169, loss = 0.47586229\n",
      "Iteration 170, loss = 0.47737743\n",
      "Iteration 171, loss = 0.47804796\n",
      "Iteration 172, loss = 0.47457573\n",
      "Iteration 173, loss = 0.47518803\n",
      "Iteration 174, loss = 0.47205109\n",
      "Iteration 175, loss = 0.47404038\n",
      "Iteration 176, loss = 0.47469399\n",
      "Iteration 177, loss = 0.46963340\n",
      "Iteration 178, loss = 0.46910830\n",
      "Iteration 179, loss = 0.46786044\n",
      "Iteration 180, loss = 0.46802145\n",
      "Iteration 181, loss = 0.46862167\n",
      "Iteration 182, loss = 0.46605405\n",
      "Iteration 183, loss = 0.46735788\n",
      "Iteration 184, loss = 0.47055488\n",
      "Iteration 185, loss = 0.46776482\n",
      "Iteration 186, loss = 0.46783945\n",
      "Iteration 187, loss = 0.47612222\n",
      "Iteration 188, loss = 0.47009701\n",
      "Iteration 189, loss = 0.47249877\n",
      "Iteration 190, loss = 0.47179140\n",
      "Iteration 191, loss = 0.46532735\n",
      "Iteration 192, loss = 0.47530951\n",
      "Iteration 193, loss = 0.46618986\n",
      "Iteration 194, loss = 0.47308650\n",
      "Iteration 195, loss = 0.46825184\n",
      "Iteration 196, loss = 0.46695959\n",
      "Iteration 197, loss = 0.46640143\n",
      "Iteration 198, loss = 0.47380028\n",
      "Iteration 199, loss = 0.46184473\n",
      "Iteration 200, loss = 0.47503376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomastesselaar/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_x, test_y, train_x, train_y = get_data()\n",
    "score, preds, test_y = predict(test_x, test_y, train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0fc5ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5846153846153846\n",
      "[ 1. -1. -1. -1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1. -1. -1.  1.  1.\n",
      " -1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1.  1.  1.  1. -1.\n",
      "  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1.  1.  1. -1.  1. -1.  1.  1.  1.  1. -1. -1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.\n",
      "  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.  1. -1.  1. -1. -1.  1.  1. -1.\n",
      "  1.  1. -1.  1.]\n",
      "[ 1. -1.  1. -1.  1.  1.  1. -1.  1.  1.  1. -1. -1.  1.  1.  1.  1. -1.\n",
      "  1.  1. -1.  1. -1.  1.  1. -1.  1. -1. -1.  1.  1.  1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1. -1. -1. -1. -1.  1.  1.  1.  1. -1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1. -1.  1.  1.  1.  1.  1. -1.\n",
      "  1.  1.  1.  1. -1.  1. -1.  1. -1. -1. -1.  1. -1. -1. -1.  1.  1. -1.\n",
      "  1. -1. -1.  1.  1.  1.  1. -1.  1. -1. -1. -1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1. -1. -1.  1. -1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1.  1. -1.\n",
      "  1. -1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "print(score)\n",
    "print(preds)\n",
    "print(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ab04e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.74767299\n",
      "Iteration 2, loss = 0.71061682\n",
      "Iteration 3, loss = 0.69355844\n",
      "Iteration 4, loss = 0.69014561\n",
      "Iteration 5, loss = 0.69219691\n",
      "Iteration 6, loss = 0.68794611\n",
      "Iteration 7, loss = 0.68296258\n",
      "Iteration 8, loss = 0.67997761\n",
      "Iteration 9, loss = 0.67883213\n",
      "Iteration 10, loss = 0.67791458\n",
      "Iteration 11, loss = 0.67696039\n",
      "Iteration 12, loss = 0.67513719\n",
      "Iteration 13, loss = 0.67289802\n",
      "Iteration 14, loss = 0.67076557\n",
      "Iteration 15, loss = 0.66934647\n",
      "Iteration 16, loss = 0.66784871\n",
      "Iteration 17, loss = 0.66747481\n",
      "Iteration 18, loss = 0.66630475\n",
      "Iteration 19, loss = 0.66476118\n",
      "Iteration 20, loss = 0.66284004\n",
      "Iteration 21, loss = 0.66132588\n",
      "Iteration 22, loss = 0.66038336\n",
      "Iteration 23, loss = 0.65787165\n",
      "Iteration 24, loss = 0.65642529\n",
      "Iteration 25, loss = 0.65449278\n",
      "Iteration 26, loss = 0.65343270\n",
      "Iteration 27, loss = 0.65160058\n",
      "Iteration 28, loss = 0.64982216\n",
      "Iteration 29, loss = 0.64757206\n",
      "Iteration 30, loss = 0.64611379\n",
      "Iteration 31, loss = 0.64347104\n",
      "Iteration 32, loss = 0.64197122\n",
      "Iteration 33, loss = 0.63934969\n",
      "Iteration 34, loss = 0.63750970\n",
      "Iteration 35, loss = 0.63577973\n",
      "Iteration 36, loss = 0.63239571\n",
      "Iteration 37, loss = 0.62997489\n",
      "Iteration 38, loss = 0.62795922\n",
      "Iteration 39, loss = 0.62485185\n",
      "Iteration 40, loss = 0.62198767\n",
      "Iteration 41, loss = 0.61960780\n",
      "Iteration 42, loss = 0.61783402\n",
      "Iteration 43, loss = 0.61370325\n",
      "Iteration 44, loss = 0.61132781\n",
      "Iteration 45, loss = 0.60910709\n",
      "Iteration 46, loss = 0.60658926\n",
      "Iteration 47, loss = 0.60451374\n",
      "Iteration 48, loss = 0.60137267\n",
      "Iteration 49, loss = 0.60064356\n",
      "Iteration 50, loss = 0.59864919\n",
      "Iteration 51, loss = 0.59638554\n",
      "Iteration 52, loss = 0.59520690\n",
      "Iteration 53, loss = 0.59098312\n",
      "Iteration 54, loss = 0.59198771\n",
      "Iteration 55, loss = 0.58767106\n",
      "Iteration 56, loss = 0.58524630\n",
      "Iteration 57, loss = 0.58079770\n",
      "Iteration 58, loss = 0.58073519\n",
      "Iteration 59, loss = 0.57821920\n",
      "Iteration 60, loss = 0.57576578\n",
      "Iteration 61, loss = 0.57460412\n",
      "Iteration 62, loss = 0.57006602\n",
      "Iteration 63, loss = 0.56895525\n",
      "Iteration 64, loss = 0.56806733\n",
      "Iteration 65, loss = 0.56477967\n",
      "Iteration 66, loss = 0.56168157\n",
      "Iteration 67, loss = 0.55929168\n",
      "Iteration 68, loss = 0.55776576\n",
      "Iteration 69, loss = 0.55541935\n",
      "Iteration 70, loss = 0.55339503\n",
      "Iteration 71, loss = 0.55260012\n",
      "Iteration 72, loss = 0.54882956\n",
      "Iteration 73, loss = 0.55048049\n",
      "Iteration 74, loss = 0.54425680\n",
      "Iteration 75, loss = 0.54143448\n",
      "Iteration 76, loss = 0.53937810\n",
      "Iteration 77, loss = 0.53583702\n",
      "Iteration 78, loss = 0.53315974\n",
      "Iteration 79, loss = 0.53306249\n",
      "Iteration 80, loss = 0.53048329\n",
      "Iteration 81, loss = 0.52883949\n",
      "Iteration 82, loss = 0.52717192\n",
      "Iteration 83, loss = 0.52674881\n",
      "Iteration 84, loss = 0.52294813\n",
      "Iteration 85, loss = 0.52239506\n",
      "Iteration 86, loss = 0.51829641\n",
      "Iteration 87, loss = 0.52045441\n",
      "Iteration 88, loss = 0.51674605\n",
      "Iteration 89, loss = 0.51824348\n",
      "Iteration 90, loss = 0.51177721\n",
      "Iteration 91, loss = 0.51345500\n",
      "Iteration 92, loss = 0.51185286\n",
      "Iteration 93, loss = 0.50990798\n",
      "Iteration 94, loss = 0.50500791\n",
      "Iteration 95, loss = 0.50925554\n",
      "Iteration 96, loss = 0.50748705\n",
      "Iteration 97, loss = 0.50423458\n",
      "Iteration 98, loss = 0.50317990\n",
      "Iteration 99, loss = 0.50259919\n",
      "Iteration 100, loss = 0.49744807\n",
      "Iteration 101, loss = 0.49695542\n",
      "Iteration 102, loss = 0.49414305\n",
      "Iteration 103, loss = 0.49376677\n",
      "Iteration 104, loss = 0.49195625\n",
      "Iteration 105, loss = 0.48925239\n",
      "Iteration 106, loss = 0.48821643\n",
      "Iteration 107, loss = 0.48832939\n",
      "Iteration 108, loss = 0.48823328\n",
      "Iteration 109, loss = 0.48300463\n",
      "Iteration 110, loss = 0.48474542\n",
      "Iteration 111, loss = 0.48526512\n",
      "Iteration 112, loss = 0.48203182\n",
      "Iteration 113, loss = 0.48254268\n",
      "Iteration 114, loss = 0.47667461\n",
      "Iteration 115, loss = 0.47994144\n",
      "Iteration 116, loss = 0.47549176\n",
      "Iteration 117, loss = 0.47463338\n",
      "Iteration 118, loss = 0.47277641\n",
      "Iteration 119, loss = 0.46909389\n",
      "Iteration 120, loss = 0.47304910\n",
      "Iteration 121, loss = 0.47436603\n",
      "Iteration 122, loss = 0.46723576\n",
      "Iteration 123, loss = 0.46920842\n",
      "Iteration 124, loss = 0.46480829\n",
      "Iteration 125, loss = 0.46945729\n",
      "Iteration 126, loss = 0.46319022\n",
      "Iteration 127, loss = 0.46551522\n",
      "Iteration 128, loss = 0.46708536\n",
      "Iteration 129, loss = 0.45815139\n",
      "Iteration 130, loss = 0.46229668\n",
      "Iteration 131, loss = 0.45751156\n",
      "Iteration 132, loss = 0.45712563\n",
      "Iteration 133, loss = 0.45896311\n",
      "Iteration 134, loss = 0.45610741\n",
      "Iteration 135, loss = 0.45274871\n",
      "Iteration 136, loss = 0.45458020\n",
      "Iteration 137, loss = 0.45585926\n",
      "Iteration 138, loss = 0.45232002\n",
      "Iteration 139, loss = 0.45299615\n",
      "Iteration 140, loss = 0.45112031\n",
      "Iteration 141, loss = 0.44792893\n",
      "Iteration 142, loss = 0.44711040\n",
      "Iteration 143, loss = 0.44793216\n",
      "Iteration 144, loss = 0.44520978\n",
      "Iteration 145, loss = 0.44516893\n",
      "Iteration 146, loss = 0.44348698\n",
      "Iteration 147, loss = 0.44109736\n",
      "Iteration 148, loss = 0.44173442\n",
      "Iteration 149, loss = 0.44060645\n",
      "Iteration 150, loss = 0.44233353\n",
      "Iteration 151, loss = 0.43873770\n",
      "Iteration 152, loss = 0.43656352\n",
      "Iteration 153, loss = 0.43808990\n",
      "Iteration 154, loss = 0.43525361\n",
      "Iteration 155, loss = 0.43634766\n",
      "Iteration 156, loss = 0.43376212\n",
      "Iteration 157, loss = 0.43201665\n",
      "Iteration 158, loss = 0.43196222\n",
      "Iteration 159, loss = 0.42938264\n",
      "Iteration 160, loss = 0.42847753\n",
      "Iteration 161, loss = 0.42777763\n",
      "Iteration 162, loss = 0.42785727\n",
      "Iteration 163, loss = 0.42430081\n",
      "Iteration 164, loss = 0.42464681\n",
      "Iteration 165, loss = 0.42627706\n",
      "Iteration 166, loss = 0.42314263\n",
      "Iteration 167, loss = 0.42535393\n",
      "Iteration 168, loss = 0.42203956\n",
      "Iteration 169, loss = 0.42242148\n",
      "Iteration 170, loss = 0.42039457\n",
      "Iteration 171, loss = 0.41962732\n",
      "Iteration 172, loss = 0.41910427\n",
      "Iteration 173, loss = 0.41843690\n",
      "Iteration 174, loss = 0.41734989\n",
      "Iteration 175, loss = 0.41432035\n",
      "Iteration 176, loss = 0.41704040\n",
      "Iteration 177, loss = 0.41295063\n",
      "Iteration 178, loss = 0.41317330\n",
      "Iteration 179, loss = 0.41325053\n",
      "Iteration 180, loss = 0.41081846\n",
      "Iteration 181, loss = 0.41491777\n",
      "Iteration 182, loss = 0.41320304\n",
      "Iteration 183, loss = 0.40940833\n",
      "Iteration 184, loss = 0.41101949\n",
      "Iteration 185, loss = 0.40926950\n",
      "Iteration 186, loss = 0.40655832\n",
      "Iteration 187, loss = 0.40837539\n",
      "Iteration 188, loss = 0.40664653\n",
      "Iteration 189, loss = 0.40558004\n",
      "Iteration 190, loss = 0.40645076\n",
      "Iteration 191, loss = 0.40143639\n",
      "Iteration 192, loss = 0.40331908\n",
      "Iteration 193, loss = 0.40308904\n",
      "Iteration 194, loss = 0.40080059\n",
      "Iteration 195, loss = 0.40223915\n",
      "Iteration 196, loss = 0.40280522\n",
      "Iteration 197, loss = 0.40057174\n",
      "Iteration 198, loss = 0.39797903\n",
      "Iteration 199, loss = 0.39940353\n",
      "Iteration 200, loss = 0.39666475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomastesselaar/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8200514138817481"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = fit_data(train_x, train_y)\n",
    "ypred = clf.predict(train_x)\n",
    "accuracy_score(train_y, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b64e18cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5076923076923077"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = clf.predict(test_x)\n",
    "accuracy_score(test_y, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42bd68eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  1.,  1., -1., -1.,  1., -1., -1.,  1., -1.,  1., -1., -1.,\n",
       "        1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,\n",
       "        1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,\n",
       "       -1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1., -1.,  1.,  1.,\n",
       "       -1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1.,\n",
       "        1., -1., -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.,  1., -1.,\n",
       "        1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
       "        1., -1., -1., -1., -1., -1.,  1., -1., -1.,  1., -1., -1., -1.,\n",
       "       -1.,  1., -1., -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b526213f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,\n",
       "        1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1.,\n",
       "       -1.,  1., -1., -1., -1., -1.,  1.,  1., -1.,  1., -1., -1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1., -1., -1., -1.,  1.,\n",
       "        1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1.,\n",
       "       -1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1., -1., -1., -1., -1.,\n",
       "        1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
       "       -1.,  1., -1.,  1., -1.,  1.,  1., -1., -1., -1.,  1., -1., -1.,\n",
       "       -1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1., -1., -1., -1., -1.,\n",
       "        1.,  1.,  1.,  1.,  1., -1., -1., -1., -1., -1., -1., -1.,  1.,\n",
       "        1.,  1.,  1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1.,\n",
       "        1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
       "       -1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1.,\n",
       "       -1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1., -1., -1., -1.,\n",
       "       -1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
       "        1., -1., -1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1.,\n",
       "        1.,  1., -1.,  1.,  1., -1.,  1., -1.,  1., -1., -1., -1.,  1.,\n",
       "       -1.,  1., -1., -1., -1.,  1., -1.,  1., -1.,  1., -1., -1., -1.,\n",
       "        1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1., -1.,  1., -1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
       "       -1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1., -1., -1.,  1.,\n",
       "       -1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1., -1.,\n",
       "       -1., -1.,  1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,  1., -1.,\n",
       "        1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "       -1.,  1., -1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1., -1.,\n",
       "       -1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,  1., -1., -1.,  1.,\n",
       "       -1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1., -1., -1., -1., -1.,\n",
       "       -1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39dd2d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [-1.,  1.,  1., -1., -1.,  1., -1., -1.,  1., -1.,  1., -1., -1.,\n",
    "        1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,\n",
    "        1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,\n",
    "       -1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1., -1.,  1.,  1.,\n",
    "       -1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1.,\n",
    "        1., -1., -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.,  1., -1.,\n",
    "        1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
    "        1., -1., -1., -1., -1., -1.,  1., -1., -1.,  1., -1., -1., -1.,\n",
    "       -1.,  1., -1., -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1.,\n",
    "        1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.]\n",
    "y = [ 1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,\n",
    "        1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1.,\n",
    "       -1.,  1., -1., -1., -1., -1.,  1.,  1., -1.,  1., -1., -1.,  1.,\n",
    "        1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1., -1., -1., -1.,  1.,\n",
    "        1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1.,\n",
    "       -1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1., -1., -1., -1., -1.,\n",
    "        1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
    "       -1.,  1., -1.,  1., -1.,  1.,  1., -1., -1., -1.,  1., -1., -1.,\n",
    "       -1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1., -1., -1., -1., -1.,\n",
    "        1.,  1.,  1.,  1.,  1., -1., -1., -1., -1., -1., -1., -1.,  1.,\n",
    "        1.,  1.,  1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1.,\n",
    "        1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
    "       -1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1.,\n",
    "       -1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1., -1., -1., -1.,\n",
    "       -1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
    "        1., -1., -1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,\n",
    "        1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1.,\n",
    "        1.,  1., -1.,  1.,  1., -1.,  1., -1.,  1., -1., -1., -1.,  1.,\n",
    "       -1.,  1., -1., -1., -1.,  1., -1.,  1., -1.,  1., -1., -1., -1.,\n",
    "        1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1.,\n",
    "        1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
    "        1., -1.,  1., -1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
    "       -1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1., -1., -1.,  1.,\n",
    "       -1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1., -1.,\n",
    "       -1., -1.,  1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,  1., -1.,\n",
    "        1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
    "       -1.,  1., -1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1., -1.,\n",
    "       -1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,  1., -1., -1.,  1.,\n",
    "       -1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1., -1., -1., -1., -1.,\n",
    "       -1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43ec56b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5645472061657033\n"
     ]
    }
   ],
   "source": [
    "print((x.count(1) + y.count(1))/(len(x) + len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "984f4736",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = preds + test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "667ddbc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'count_nonzero'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1b/rzf7l3bx7j18zpl5_9n43y980000gn/T/ipykernel_48055/3085159394.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'count_nonzero'"
     ]
    }
   ],
   "source": [
    "print(z.count(2)/test_y.count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1187bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = z.tolist()\n",
    "test_y = test_y.tolist()\n",
    "preds = preds.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "32ee1189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76\n"
     ]
    }
   ],
   "source": [
    "print(z.count(2)/test_y.count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "54041a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.count(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ccc9e7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34545454545454546"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.count(-2)/test_y.count(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0838d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
